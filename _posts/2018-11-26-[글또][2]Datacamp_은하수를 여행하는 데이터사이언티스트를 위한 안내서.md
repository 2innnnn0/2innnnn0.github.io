<!-- _posts/2018-11-26-[글또][2]Datacamp_은하수를 여행하는 데이터사이언티스트를 위한 안내서.md -->

![tyle_글또](https://dl.dropbox.com/s/ks5nuq6pwll7pel/tyle-SLO-8-1543035462.png)

## Datacamp 은하수를 여행하는 데이터사이언티스트를 위한 안내서

데이터 캠프는 웹기반 IDE로 구성되어있는 실습형 프로그래밍 교육서비스입니다.
Python & R 을 주로 교육과정이 이루어져 있고, 제목처럼 입문자들이 쉽게 들어갈 수 있게 잘 코스가 짜여있습니다.
하나의 단계별로 대체로 10~20줄 미만의 코드로 되어있기 때문에 학습 속도도 매우 빠릅니다.

틈 날때마다 강의 내용을 올려드리려 합니다. 이론 설명보다는 코드 중심으로 작성됨을 말씀드립니다.



오늘 다룰 코스는 sklearn으로 하는 '지도학습supervised learning' 입니다.

< 강의 슬라이드 번역본 >

우선 머신러닝이란 무엇인가?, 데이터로 부터 의사결정을 할 수 있도록 컴퓨터에게 학습할 능력을 주는 것 입니다.
예를 들어, 이메일과 스팸을 구분하는 능력 혹은 위키피디아 비슷한 글들을 서로 묶어주는 것 입니다.
머신러닝은 아래 두가지로 나뉩니다.
지도학습이란, 목표가 정해진 #레이블(labeled)이 있는 데이터 사용한 학습
비지도학습이란, 레이블이 없는(unlabeled) 데어틀 사용하는 학습 입니다.
(#흔히 종속변수 또는 타겟변수, 목표변수라 불리는 label을 우리가 알고 싶어하는 값이라고 생각하면 됩니다.)

파이썬으로 하는 지도학습 방법으로 여기서는 scikit-learn 사용하지만 tensorflow, keras 등도 있습니다.


## 1. Classification
1984년 미국 의회의 투표 기록을 집계한 데이터입니다.
실습 데이터는 하단 링크로 다운 받으시면 됩니다.
[US Congressional Voting Records (1984) Dataset](https://assets.datacamp.com/production/course_1939/datasets/house-votes-84.csv)
EDA를 통해 같은 당원이라 판단되는 값들을 묶어주는 k-Nearest Neighbors 를 사용할 겁니다.

[party] -> republican 과 democrat 둘 중 하나를 예측하는 데이터입니다.
ex. array(['republican', 'republican', 'democrat', 'democrat', 'democrat',
       'democrat', 'democrat', 'republican', 'republican', 'democrat']

### [1-1. k-Nearest Neighbors: Fit ]
```python
# 1. Knn을 할 수 있도록 패키지를 불러옵니다.
from sklearn.neighbors import KNeighborsClassifier

# 2. X: feature 변수, Y: label 컬럼을 구분하여 저장합니다.
y = df['party'].values # label 어느 정당인가?
X = df.drop('party', axis=1).values # label 뺀 나머지 컬럼값

# 3. 6개의 이웃 노드로 하는 Knn 구별자 모델을 만듭니다
knn = KNeighborsClassifier(n_neighbors = 6)

# 4. 데이터를 위에서 만든 모델에 적용합니다.
knn.fit(X,y)

# 짜잔 결과가 나왔습니다.
>> KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=6, p=2,
           weights='uniform')
```

이제는 한단계 더 내려가, 만든 모델에 새로운 데이터(X_new)를 넣어봅니다.
X_new에서 `.predict()` 는 1개의 예측을 생성하며, 435 개의 예측이 생성됩니다 (각 샘플마다 1 개씩)
(X_new는 아래를 복사해서 넣어주세요)
>> array([[ 0.1624411 ,  0.98782531,  0.10292704,  0.65930425,  0.80059352,
         0.17231305,  0.34736972,  0.65242355,  0.94535194,  0.02340574,
         0.15123243,  0.95085668,  0.71604157,  0.56008167,  0.27579899,
         0.47550731]])

### [1-2. k-Nearest Neighbors: predict ]

```python
from sklearn.neighbors import KNeighborsClassifier
y = df['party'].values
X = df.drop('party', axis=1).values
knn = KNeighborsClassifier(n_neighbors=6)
knn.fit(X,y)

# Predict the labels for the training data X
y_pred = knn.predict(X)

# Predict and print the label for the new data point X_new
new_prediction = knn.predict(X_new)
print("Prediction: {}".format(new_prediction))

# X_new는 republican으로 예측을 했습니다.
>> Prediction: ['republican']
```

일단 뭔지는 모르겠지만, 맞추었습니다!!
마지막 프린트하는것 까지 해서 단 8줄로 쉽게 예측이 가능하다는게 놀랍습니다.
좀더 자세하게 알면 좋지만, 데이터를 어떻게 학습하는지 그리고 `정말 잘 맞추는지` 를 코드를 보며 이해하도록 하겠습니다.

이번 챕터에선 train/test으로 데이터를 나눈 방법과 fit/predict/accuracy를 평가metric하는 내용을 다루겠습니다.
우선 새로운 데이터셋, 손글씨digits 를 가져와보겠습니다.
`from sklearn import datasets` 으로 가져오고, `load_digits()`을 콜 합니다.
(`datasets` 안에는 R을 해보셨으면 익숙한 iris 데이터도 있습니다.)

### [1-3 The digits recognition dataset]
```python
# Import necessary modules
from sklearn import datasets
import matplotlib.pyplot as plt

# Load the digits dataset: digits
digits = datasets.load_digits()

# Print the keys and DESCR of the dataset
print(digits.keys())
print(digits.DESCR) # 데이터셋에 대한 설명이 있습니다

# Print the shape of the images and data keys
print(digits.images.shape) # 이미지는 1797개, 8*8픽셀로 이루어진 이미지입니다.
print(digits.data.shape)

# Display digit 1010
plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')
plt.show()

# >> 숫자 5가 출력됩니다.
```

다룰 데이터를 확인했으니, 이제 앞에서 만든 모델을 다시 한번 만들어봅시다.
`train_test_split()` 으로 데이터를 test_size 는 20%로 설정합니다.
`.score()` 은 accuracy를 측정하는 지표 함수입니다.

### [1-4 Train/Test Split + Fit/Predict/Accuracy ]

```python
# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split


# Create feature and target arrays
X = digits.data
y = digits.target

# Split into training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify= y )

# Create a k-NN classifier with 7 neighbors: knn
knn = KNeighborsClassifier(n_neighbors=7)

# Fit the classifier to the training data
knn.fit(X_train,y_train)

# Print the accuracy
print(knn.score(X_test, y_test))

# 98%의 정확도를 가지는 모델이 만들어졌습니다.
>>> 0.983333333333
```

knn에서 k의 개수를 통해, 데이터에 과적합 했는지, 그렇지 않았는지 보려합니다.

### [1-5 과적합 & 과소적합 Overfitting and underfitting ]

```python
# Setup arrays to store train and test accuracies
neighbors = np.arange(1, 9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))


# Loop over different values of k
for i, k in enumerate(neighbors):
    # Setup a k-NN Classifier with k neighbors: knn
    knn = KNeighborsClassifier( n_neighbors= k )

    # Fit the classifier to the training data
    knn.fit(X_train, y_train)

    #Compute accuracy on the training set
    train_accuracy[i] = knn.score(X_train, y_train)

    #Compute accuracy on the testing set
    test_accuracy[i] = knn.score(X_test, y_test)

# Generate plot
plt.title('k-NN: Varying Number of Neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.show()

```
여기까지 '분류'를 학습하는 과정을 배웠습니다.
다음은 '회귀' 학습 입니다.



## 2. Regression

갭마인더 데이터셋을 불러옵니다.
[gapminder](https://assets.datacamp.com/production/course_1939/datasets/gm_2008_region.csv)
[갭마인더](https://www.gapminder.org/data/) 는 한스로슬링의 년도별 인구변화를 보여준 TED강의로 유명합니다.
관련 영상을 링크로 남겨두었습니다.(https://www.youtube.com/watch?v=hVimVzgtD6w)

### [2-1. 데이터 불러오기 Importing data for supervised learning]
```python
# Import numpy and pandas
import numpy as np
import pandas as pd

# Read the CSV file into a DataFrame: df
df = pd.read_csv('gapminder.csv')

# Create arrays for features and target variable
y = df['life']
X = df['fertility']

# Print the dimensions of X and y before reshaping
print("Dimensions of y before reshaping: {}".format(y.shape))
print("Dimensions of X before reshaping: {}".format(X.shape))

# reshape를 통해, 분석하기 좋게 데이터를 변형합니다.
# Reshape X and y
y = y.reshape(-1,1)
X = X.reshape(-1,1)

# Print the dimensions of X and y after reshaping
print("Dimensions of y after reshaping: {}".format(y.shape))
print("Dimensions of X after reshaping: {}".format(X.shape))

```



### [2-2. 회귀 모델 생성 및 예측하기 Fit & predict for regression ]


fertility 와 life의 상관성을 corr로 확인해보면
강한 음의 상관관계를 가지는 것으로 나옵니다.
그렇다면 fertility로 life를 알 수 있을 것이라 판단됩니다.
`sns.heatmap(df.corr(), square=True, cmap='RdYlGn')`


``` python
# Import LinearRegression
from sklearn.linear_model import LinearRegression

# Create the regressor: reg
reg = LinearRegression()

# Create the prediction space
prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)


# Fit the model to the data
reg.fit(X_fertility, y)

# Compute predictions over the prediction space: y_pred
y_pred = reg.predict(prediction_space)

# Print R^2
print(reg.score(X_fertility, y))

# Plot regression line
plt.plot(prediction_space, y_pred, color='black', linewidth=3)
plt.show()

```



### [2-3. 학습/테스트 데이터 분리하기. Train/test split for regression]
```python
# Import necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)

# Create the regressor: reg_all
reg_all = LinearRegression()

# Fit the regressor to the training data
reg_all.fit(X_train, y_train)

# Predict on the test data: y_pred
y_pred = reg_all.predict(X_test)

# Compute and print R^2 and RMSE
print("R^2: {}".format(reg_all.score(X_test, y_test)))
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error: {}".format(rmse))

```




### [2-4. 5-fold cross-validation]
```python
# Import the necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score


# df.info()
# Create a linear regression object: reg
reg = LinearRegression()

# Compute 5-fold cross-validation scores: cv_scores
cv_scores = cross_val_score( reg, X, y, cv=5  )


# Print the 5-fold cross-validation scores
print(cv_scores)
>> [ 0.81720569  0.82917058  0.90214134  0.80633989  0.94495637]
print("Average 5-Fold CV Score: {}".format(np.mean(cv_scores)))
>> Average 5-Fold CV Score: 0.8599627722793232

```

### [2-5. K-Fold CV comparison]

```python
# Import necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# Create a linear regression object: reg
reg = LinearRegression()

# Perform 3-fold CV
cvscores_3 = cross_val_score(reg, X,y , cv=3)
print(np.mean(cvscores_3))

# Perform 10-fold CV
cvscores_10 = cross_val_score(reg, X,y ,cv=10)
print(np.mean(cvscores_10))

```


### [2-6. Lasso 정규화. Regularization I: Lasso ]

```python
# Import Lasso
from sklearn.linear_model import Lasso

# Instantiate a lasso regressor: lasso
lasso = Lasso( alpha=0.4, normalize=True )

# Fit the regressor to the data
lasso.fit(X,y)

# Compute and print the coefficients
lasso_coef = lasso.coef_
print(lasso_coef)

# Plot the coefficients
plt.plot(range(len(df_columns)), lasso_coef)
plt.xticks(range(len(df_columns)), df_columns.values, rotation=60)
plt.margins(0.02)
plt.show()

```


### [2-7. Ridge 정규화, Regularization II: Ridge]

```python
# Import necessary modules
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# Setup the array of alphas and lists to store scores
alpha_space = np.logspace(-4, 0, 50)
ridge_scores = []
ridge_scores_std = []

# Create a ridge regressor: ridge
ridge = Ridge( normalize= True )

# Compute scores over range of alphas
for alpha in alpha_space:

    # Specify the alpha value to use: ridge.alpha
    ridge.alpha = alpha

    # Perform 10-fold CV: ridge_cv_scores
    ridge_cv_scores = cross_val_score(ridge, X,y, cv=10)

    # Append the mean of ridge_cv_scores to ridge_scores
    ridge_scores.append(np.mean(ridge_cv_scores))

    # Append the std of ridge_cv_scores to ridge_scores_std
    ridge_scores_std.append(np.std(ridge_cv_scores))

# Display the plot
display_plot(ridge_scores, ridge_scores_std)

```
